# backend.py
from fastapi import FastAPI, Request, Response, HTTPException
from fastapi.responses import JSONResponse, FileResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field, ConfigDict
from typing import Optional, Dict, Any, AsyncGenerator
import aiohttp
import json
from openai import OpenAI
import os
import hashlib
from dotenv import load_dotenv
import time
from google import genai
import logging
from functools import wraps
from pathlib import Path
import uvicorn
import asyncio

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# åˆå§‹åŒ–å¤©æ°”ç¼“å­˜å­—å…¸
weather_cache: Dict[str, Any] = {}

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="Weather AI Assistant",
    description="An AI-powered weather assistant API",
    version="1.0.0",
)

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ç¯å¢ƒå˜é‡
METEOBLUE_API_KEY = os.getenv("METEOBLUE_API_KEY")
WAQI_TOKEN = os.getenv("WAQI_TOKEN")
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
BAIDU_MAP_AK = os.getenv("BAIDU_MAP_AK")
BAIDU_MAP_SK = os.getenv("BAIDU_MAP_SK")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# APIå¯†é’¥éªŒè¯
required_keys = {
    'METEOBLUE_API_KEY': METEOBLUE_API_KEY,
    'WAQI_TOKEN': WAQI_TOKEN,
    'DEEPSEEK_API_KEY': DEEPSEEK_API_KEY,
    'BAIDU_MAP_AK': BAIDU_MAP_AK,
    'BAIDU_MAP_SK': BAIDU_MAP_SK,
    'GEMINI_API_KEY': GEMINI_API_KEY
}

missing_keys = [key for key, value in required_keys.items() if not value]
if missing_keys:
    raise ValueError(f"Missing API keys: {', '.join(missing_keys)}. Please set them in your .env file.")

# åˆå§‹åŒ–AIå®¢æˆ·ç«¯
deepseek_client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url="https://api.siliconflow.cn/v1/")
gemini_client = genai.Client(api_key=GEMINI_API_KEY)

# è¯·æ±‚æ¨¡å‹
class WeatherRequest(BaseModel):
    model_config = ConfigDict(protected_namespaces=())
    
    latitude: float = Field(..., ge=-90, le=90)
    longitude: float = Field(..., ge=-180, le=180)
    query: str = Field(..., min_length=1)
    forecast_days: Optional[int] = Field(5, ge=1, le=14)
    model_type: Optional[str] = Field("gemini", pattern="^(gemini|deepseek)$")

class FollowupRequest(BaseModel):
    model_config = ConfigDict(protected_namespaces=())
    
    latitude: float = Field(..., ge=-90, le=90)
    longitude: float = Field(..., ge=-180, le=180)
    query: str = Field(..., min_length=1)
    forecast_days: Optional[int] = Field(5, ge=1, le=14)
    model_type: Optional[str] = Field("gemini", pattern="^(gemini|deepseek)$")

class CityRequest(BaseModel):
    city: str = Field(..., min_length=1)

def generate_cache_key(latitude: float, longitude: float, forecast_days: int, model_type: str) -> str:
    key_string = f"{latitude}_{longitude}_{forecast_days}_{model_type}"
    return hashlib.md5(key_string.encode()).hexdigest()

# é™æ€æ–‡ä»¶æœåŠ¡
app.mount("/static", StaticFiles(directory="static"), name="static")

@app.get("/")
async def read_root():
    return FileResponse("static/index.html")

@app.get("/sw.js")
async def get_sw():
    return FileResponse("static/sw.js", media_type="application/javascript")

@app.post("/get_weather_advice")
async def get_weather_advice(request: Request, weather_request: WeatherRequest):
    try:
        # æ·»åŠ æç¤ºè¯­åˆ°æŸ¥è¯¢
        enhanced_query = weather_request.query + '''
è¯·ä»¥å‹å¥½è€Œä¸“ä¸šçš„æ–¹å¼å›ç­”ï¼Œå°†ç»“æ„åŒ–ä¿¡æ¯èå…¥è‡ªç„¶å¯¹è¯ä¸­ã€‚

ğŸ¯ åŸºæœ¬æ¡†æ¶ï¼š
1. å¼€åœºç™½ï¼ˆç®€çŸ­å‹å¥½ï¼‰ï¼š
   - é—®å€™è¯­
   - ğŸ“ ä½ç½®ä¿¡æ¯ï¼š[ç»åº¦ï¼Œçº¬åº¦] - [åœ°ç†åç§°]
   - å¤©æ°”æ¦‚è§ˆï¼šç”¨ä¸€å¥è¯æ¦‚æ‹¬å½“å‰å¤©æ°”ç‰¹ç‚¹

2. æ ¸å¿ƒå¤©æ°”ä¿¡æ¯ï¼ˆåˆ†ç±»å‘ˆç°ï¼‰ï¼š
   â° å®æ—¶å¤©æ°”ï¼š
   æ¸©åº¦ï¼š[æ•°å€¼]Â°C
   ä½“æ„Ÿï¼š[æ•°å€¼]Â°C
   æ¹¿åº¦ï¼š[æ•°å€¼]%
   é£å‘ï¼š[æ–¹å‘] [é£é€Ÿ]m/s
   
   ğŸŒ¡ï¸ ä»Šæ—¥æ¸©å·®ï¼š
   æœ€é«˜æ¸©ï¼š[æ•°å€¼]Â°C
   æœ€ä½æ¸©ï¼š[æ•°å€¼]Â°C
   æ¸©å·®ï¼š[æ•°å€¼]Â°C

   ğŸ’¨ ç©ºæ°”çŠ¶å†µï¼š
   AQIæŒ‡æ•°ï¼š[æ•°å€¼]
   ä¸»è¦æ±¡æŸ“ç‰©ï¼š[åç§°]
   ç©ºæ°”è´¨é‡ï¼š[ç­‰çº§æè¿°]

3. ğŸ“… æœªæ¥å¤©æ°”é¢„æŠ¥ï¼ˆæŒ‰æ—¥æœŸåˆ†æ®µï¼‰ï¼š
   "YYYY-MM-DD"
   - å¤©æ°”ç‰¹å¾ï¼š[æè¿°]
   - æ¸©åº¦åŒºé—´ï¼š[æœ€ä½æ¸©]-[æœ€é«˜æ¸©]Â°C
   - é™æ°´æ¦‚ç‡ï¼š[æ•°å€¼]%
   - å…³é”®æé†’ï¼š[é‡ç‚¹ä¿¡æ¯]
   ---

4. ğŸ‘” å®ç”¨å»ºè®®ï¼š
   > ç©¿è¡£å»ºè®®ï¼š[å…·ä½“å»ºè®®]
   > å‡ºè¡Œå»ºè®®ï¼š[å…·ä½“å»ºè®®]
   > æ´»åŠ¨å»ºè®®ï¼š[å…·ä½“å»ºè®®]

5. âš ï¸ ç‰¹åˆ«æé†’ï¼ˆå¦‚æœ‰ï¼‰ï¼š
   - æç«¯å¤©æ°”é¢„è­¦
   - ç‰¹æ®Šå¤©æ°”æ³¨æ„äº‹é¡¹
   - å¥åº·é˜²æŠ¤å»ºè®®

6. ç»“è¯­ï¼š
   - æ¸©é¦¨æç¤ºæˆ–ç¥ç¦è¯­
   - è¡¨è¾¾å…³å¿ƒ

è¯·æ³¨æ„ï¼š
1. åœ¨ä¿æŒç»“æ„æ¸…æ™°çš„åŒæ—¶ï¼Œç”¨è‡ªç„¶çš„è¯­è¨€è¿æ¥å„éƒ¨åˆ†
2. é‡è¦æ•°æ®è¦ç”¨"å¼•å·"æ ‡æ³¨
3. å…³é”®æé†’ä½¿ç”¨ > ç¬¦å·
4. åœ¨åˆé€‚çš„åœ°æ–¹ä½¿ç”¨emojiå¢åŠ å¯è¯»æ€§
5. æç«¯å¤©æ°”ä½¿ç”¨ âš ï¸ çªå‡ºæ˜¾ç¤º
6. ç”¨åˆ†éš”çº¿(---)åŒºåˆ†ä¸åŒæ—¥æœŸçš„é¢„æŠ¥

è¯·ç”¨æ¸©å’Œå‹å¥½çš„è¯­æ°”è¾“å‡ºï¼Œä½†ç¡®ä¿ä¿¡æ¯å®Œæ•´ä¸”ç»“æ„æ¸…æ™°ã€‚'''
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = generate_cache_key(
            weather_request.latitude,
            weather_request.longitude,
            weather_request.forecast_days,
            weather_request.model_type
        )

        # ç¡®ä¿å¤©æ•°åœ¨æœ‰æ•ˆèŒƒå›´å†…
        forecast_days = max(1, min(weather_request.forecast_days, 14))

        async with aiohttp.ClientSession() as session:
            # è·å–å¤©æ°”æ•°æ®
            weather_url = f"https://my.meteoblue.com/packages/basic-3h?apikey={METEOBLUE_API_KEY}&lat={weather_request.latitude}&lon={weather_request.longitude}&format=json&windspeed=ms-1&forecast_days={forecast_days}"
            async with session.get(weather_url) as weather_response:
                weather_response.raise_for_status()
                weather_data = await weather_response.json()

            # è·å–ç©ºæ°”è´¨é‡æ•°æ®
            air_quality_url = f"https://api.waqi.info/feed/geo:{weather_request.latitude};{weather_request.longitude}/?token={WAQI_TOKEN}"
            async with session.get(air_quality_url) as air_quality_response:
                air_quality_response.raise_for_status()
                air_quality_data = await air_quality_response.json()

        # å‡†å¤‡LLMä¸Šä¸‹æ–‡
        context = f"""
        Weather Data (for {forecast_days} days): {json.dumps(weather_data)}
        Air Quality Data: {json.dumps(air_quality_data)}
        User Location: Latitude {weather_request.latitude}, Longitude {weather_request.longitude}
        """

        async def generate_stream() -> AsyncGenerator[str, None]:
            try:
                if weather_request.model_type == 'deepseek':
                    stream = deepseek_client.chat.completions.create(
                        model="deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
                        messages=[{"role": "user", "content": f"{context}\n\nUser Query: {enhanced_query}"}],
                        stream=True
                    )
                    content = ""
                    reasoning_content = ""
                    try:
                        for chunk in stream:
                            if await request.is_disconnected():
                                logger.info("Client disconnected, stopping stream")
                                return
                            if chunk.choices[0].delta.content:
                                content = chunk.choices[0].delta.content
                                yield f"data: {json.dumps({'content': content})}\n\n"
                            if hasattr(chunk.choices[0].delta, 'reasoning_content') and chunk.choices[0].delta.reasoning_content:
                                reasoning_content = chunk.choices[0].delta.reasoning_content
                                yield f"data: {json.dumps({'reasoning_content': reasoning_content})}\n\n"
                    except ConnectionResetError:
                        logger.warning("Connection reset by client")
                        return
                    except Exception as e:
                        logger.error(f"Stream processing error: {str(e)}")
                        raise
                elif weather_request.model_type == 'gemini':
                    stream = gemini_client.models.generate_content_stream(
                        model="gemini-2.0-flash-thinking-exp-01-21",
                        contents=f"{context}\n\nUser Query: {enhanced_query}"
                    )
                    started = False
                    try:
                        for chunk in stream:
                            if await request.is_disconnected():
                                logger.info("Client disconnected, stopping stream")
                                return
                            if not started:
                                started = True
                                yield f"data: {json.dumps({'start': True})}\n\n"
                            if chunk.text:
                                # ç¡®ä¿è¡¨æ ¼æ ¼å¼åœ¨ JSON åºåˆ—åŒ–æ—¶ä¿æŒä¸å˜
                                formatted_text = chunk.text.replace('\\n', '\n').replace('\\t', '\t')
                                yield f"data: {json.dumps({'content': formatted_text, 'preserve_format': True})}\n\n"
                        if not await request.is_disconnected():
                            yield "data: {\"type\": \"done\"}\n\n"  # ä¿®æ”¹å®Œæˆä¿¡å·çš„æ ¼å¼
                    except ConnectionResetError:
                        logger.warning("Connection reset by client")
                        return
                    except Exception as e:
                        logger.error(f"Stream processing error: {str(e)}")
                        raise
                else:
                    raise HTTPException(status_code=400, detail=f"Unsupported model type: {weather_request.model_type}")
                
                # ç§»é™¤è¿™é‡Œçš„ DONE ä¿¡å·ï¼Œå› ä¸ºå·²ç»åœ¨å„è‡ªçš„å¤„ç†åˆ†æ”¯ä¸­å‘é€äº†
                # if not await request.is_disconnected():
                #     yield "data: [DONE]\n\n"
            except Exception as e:
                logger.error(f"Streaming error: {str(e)}", exc_info=True)
                if not await request.is_disconnected():
                    yield f"data: {json.dumps({'error': str(e)})}\n\n"

        return StreamingResponse(generate_stream(), media_type="text/event-stream")

    except aiohttp.ClientError as e:
        raise HTTPException(status_code=500, detail=f"API request failed: {str(e)}")
    except Exception as e:
        logger.error(f"Error in get_weather_advice: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")

@app.post("/ask_followup")
async def handle_followup_question(request: Request, followup_request: FollowupRequest):
    try:
        enhanced_query = followup_request.query + ' è¯·ç›´æ¥è¾“å‡ºå›ç­”ã€‚ä¿æŒåŸæœ‰æ ¼å¼é£æ ¼ï¼Œä½¿ç”¨åˆ†å‰²çº¿å’Œæ˜Ÿçº§è¯„ä»·ã€‚ä¸è¦ä½¿ç”¨ä»»ä½•Markdownè¯­æ³•æ ‡è®°ã€‚'
        
        cache_key = generate_cache_key(
            followup_request.latitude,
            followup_request.longitude,
            followup_request.forecast_days,
            followup_request.model_type
        )
        
        cached_data = weather_cache.get(cache_key)
        if not cached_data:
            raise HTTPException(status_code=400, detail="è¯·å…ˆå®Œæˆåˆå§‹å¤©æ°”æŸ¥è¯¢æ‰èƒ½è¿›è¡Œè¿½é—®")

        async def generate_stream() -> AsyncGenerator[str, None]:
            try:
                if followup_request.model_type == 'deepseek':
                    messages = [{
                        "role": "user",
                        "content": f"""
                        å¤©æ°”æ•°æ®ï¼š{json.dumps(cached_data['weather_data'])}
                        ç©ºæ°”è´¨é‡æ•°æ®ï¼š{json.dumps(cached_data['air_quality_data'])}
                        ç”¨æˆ·ä½ç½®ï¼šçº¬åº¦ {followup_request.latitude}ï¼Œç»åº¦ {followup_request.longitude}

                        ç”¨æˆ·è¿½é—®ï¼š{enhanced_query}
                        """
                    }]
                    stream = deepseek_client.chat.completions.create(
                        model="deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
                        messages=messages,
                        stream=True
                    )
                    content = ""
                    reasoning_content = ""
                    try:
                        for chunk in stream:
                            if await request.is_disconnected():
                                logger.info("Client disconnected, stopping stream")
                                return
                            if chunk.choices[0].delta.content:
                                content = chunk.choices[0].delta.content
                                yield f"data: {json.dumps({'content': content})}\n\n"
                            if hasattr(chunk.choices[0].delta, 'reasoning_content') and chunk.choices[0].delta.reasoning_content:
                                reasoning_content = chunk.choices[0].delta.reasoning_content
                                yield f"data: {json.dumps({'reasoning_content': reasoning_content})}\n\n"
                    except ConnectionResetError:
                        logger.warning("Connection reset by client")
                        return
                    except Exception as e:
                        logger.error(f"Stream processing error: {str(e)}")
                        raise
                elif followup_request.model_type == 'gemini':
                    messages = f"""
                        å¤©æ°”æ•°æ®ï¼š{json.dumps(cached_data['weather_data'])}
                        ç©ºæ°”è´¨é‡æ•°æ®ï¼š{json.dumps(cached_data['air_quality_data'])}
                        ç”¨æˆ·ä½ç½®ï¼šçº¬åº¦ {followup_request.latitude}ï¼Œç»åº¦ {followup_request.longitude}

                        ç”¨æˆ·è¿½é—®ï¼š{enhanced_query}
                        """
                    stream = gemini_client.models.generate_content_stream(
                        model="gemini-2.0-flash-thinking-exp-01-21",
                        contents=messages
                    )
                    started = False
                    try:
                        for chunk in stream:
                            if await request.is_disconnected():
                                logger.info("Client disconnected, stopping stream")
                                return
                            if not started:
                                started = True
                                yield f"data: {json.dumps({'start': True})}\n\n"
                            if chunk.text:
                                # ç¡®ä¿è¡¨æ ¼æ ¼å¼åœ¨ JSON åºåˆ—åŒ–æ—¶ä¿æŒä¸å˜
                                formatted_text = chunk.text.replace('\\n', '\n').replace('\\t', '\t')
                                yield f"data: {json.dumps({'content': formatted_text, 'preserve_format': True})}\n\n"
                        if not await request.is_disconnected():
                            yield "data: {\"type\": \"done\"}\n\n"  # ä¿®æ”¹å®Œæˆä¿¡å·çš„æ ¼å¼
                    except ConnectionResetError:
                        logger.warning("Connection reset by client")
                        return
                    except Exception as e:
                        logger.error(f"Stream processing error: {str(e)}")
                        raise
                else:
                    raise HTTPException(status_code=400, detail=f"Unsupported model type: {followup_request.model_type}")
                
                # ç§»é™¤è¿™é‡Œçš„ DONE ä¿¡å·ï¼Œå› ä¸ºå·²ç»åœ¨å„è‡ªçš„å¤„ç†åˆ†æ”¯ä¸­å‘é€äº†
                # if not await request.is_disconnected():
                #     yield "data: [DONE]\n\n"
            except Exception as e:
                logger.error(f"Streaming error: {str(e)}", exc_info=True)
                if not await request.is_disconnected():
                    yield f"data: {json.dumps({'error': str(e)})}\n\n"

        return StreamingResponse(generate_stream(), media_type="text/event-stream")

    except Exception as e:
        logger.error(f"Error in handle_followup_question: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"å¤„ç†è¿½é—®æ—¶å‡ºé”™: {str(e)}")

@app.post("/get_city_location")
async def get_city_location(request: CityRequest):
    if not request.city:
        raise HTTPException(status_code=400, detail="åŸå¸‚åç§°ä¸èƒ½ä¸ºç©º")

    try:
        # æ„å»ºæç¤ºè¯ï¼Œè¦æ±‚Geminiè¿”å›ç²¾ç¡®çš„ç»çº¬åº¦
        prompt = f"""
        è¯·æä¾›{request.city}çš„ç²¾ç¡®ç»çº¬åº¦åæ ‡ã€‚
        è¦æ±‚ï¼š
        1. åªè¿”å›ç»çº¬åº¦æ•°å€¼ï¼Œæ ¼å¼ä¸ºï¼šçº¬åº¦,ç»åº¦
        2. å¦‚æœæ˜¯çœä»½ï¼Œè¿”å›çœä¼šåŸå¸‚çš„ç»çº¬åº¦
        3. å¦‚æœæ˜¯ç›´è¾–å¸‚æˆ–ç‰¹åˆ«è¡Œæ”¿åŒºï¼Œè¿”å›å…¶å¸‚ä¸­å¿ƒç»çº¬åº¦
        4. æ•°å€¼ç²¾ç¡®åˆ°å°æ•°ç‚¹å4ä½
        5. ä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—è¯´æ˜
        ç¤ºä¾‹è¿”å›æ ¼å¼ï¼š39.9042,116.4074
        """

        # ä½¿ç”¨Gemini APIè·å–ç»çº¬åº¦
        llm_response = gemini_client.models.generate_content(
            model="gemini-2.0-flash-lite",
            contents=prompt
        )

        # è§£æå“åº”
        coordinates = llm_response.text.strip().split(',')
        if len(coordinates) != 2:
            raise ValueError("Invalid coordinates format")

        latitude = float(coordinates[0])
        longitude = float(coordinates[1])

        # Validate coordinates are within reasonable global range
        if not (-90 <= latitude <= 90 and -180 <= longitude <= 180):
            raise ValueError("Invalid coordinates range")

        return JSONResponse(content={
            'latitude': latitude,
            'longitude': longitude
        })

    except (ValueError, IndexError) as e:
        raise HTTPException(status_code=400, detail=f"æ— æ³•è§£æåŸå¸‚ä½ç½®ä¿¡æ¯: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–åŸå¸‚ä½ç½®ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")

if __name__ == "__main__":
    # ç”Ÿäº§ç¯å¢ƒé…ç½®
    uvicorn_config = uvicorn.Config(
        "app:app",
        host="0.0.0.0",
        port=8000,
        workers=4,  # æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´
        loop="uvloop",
        http="httptools",
        log_level="info",
        reload=True,  # ç”Ÿäº§ç¯å¢ƒç¦ç”¨çƒ­é‡
        access_log=True,
    )
    server = uvicorn.Server(uvicorn_config)
    server.run()
